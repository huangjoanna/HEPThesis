\section{Background estimation}
\label{sec:background}

\subsection{Defining leptons}

The four lepton channel is quite the golden channel, as it has a very clean signature with minimal background. The dominant background in this analysis is when one or more of the reconstructed leptons in the quadruplet are not real leptons; rather they are misidentified objects in the detector mimicking the same signature \cite{varnes2016poisson}. These "leptons" are non-prompt, and can be referred to as a fake lepton, whereas a lepton produced from the hard scatter is a prompt, real lepton. One source of fake leptons is from hadron decays. In the case of the electron, photon conversion and hadronic jets misidentified due to their large and narrow deposit in the electromagnetic calorimeter can also play a role. In this analysis, around three-quarters of the fakes originate from \Pbottom-hadron decays in $\text{Z}+$jets and $\text{t}\Bar{\text{t}}$ events. 

The size and behaviour of the fake lepton background - also referred to as the reducible background - is usually estimated using data-driven methods because they are not well modelled by simulation \cite{varnes2016poisson}. One such method is the Fake Factor method. This method depends on two sets of lepton criteria: a tight criteria that selects leptons which make it into the signal region, and a loose criteria that is similar but less restrictive. The leptons selected by the latter are referred to as baseline leptons, and the baseline leptons that additionally pass the tight criteria are the signal leptons. The rest of this section will also touch on baseline-not-signal leptons; these are leptons that pass the "baseline" loose selection, but do not make the "signal" tight selection. 

\subsection{Fake Factor method}

The Fake Factor method relies on the calculation of a fake efficiency, $f$, which is the fraction of fake baseline leptons pass the tight selection criteria and become signal leptons. Because fake leptons not well modelled in simulation, the fake efficiency is calculated in data, in an alternative region of phase space that is enriched with fake leptons. 

Using the Fake Factor $F$, the number of baseline leptons, and the number of real baseline leptons, the number of fake signal leptons can be calculated as
\begin{equation}
    N_{\text{fake}}^{\text{signal}} = F(N^{\text{baseline}}-N^{\text{baseline}}_{\text{real}}).
\end{equation}
Note that the FF method assumes good modelling in the real component of the simulation since $N^{\text{baseline}}_{\text{real}}$ is taken from MC. The method is described in detail in Reference~\cite{ATLAS-CONF-2014-058}.
%% Smoothing

Smoothing on the raw output of the reducible background estimate is performed. The raw output, due to low statistics in certain bins, have pronounced, jagged features that resemble resonances. Of course, resonant peaks should not exist. The smoothing procedure is therefore used to obtain a more even shape, minimizing the impact of any outlier bins that had a large Fake Factor weight. In order to smooth the distribution, an intermediate, finer binning is assigned to each observable and the background estimate is run. The fine-binned intermediate background distribution is smoothed with Friedman's super smoother~\cite{DAGOSTINI1995487}. Lastly, the final background estimate is obtained by integrating over the smoothed distribution using the coarser, original binning. 

\subsection{Fake background uncertainties}
\label{ssec:fakeuncertainty}

In the fake-factor background estimate, there are five sources of uncertainty considered:
\begin{enumerate}
    \item Dominant in the low- and high-mass tails where \mFourL < 150 GeV and \mFourL > 350 GeV is the statistical uncertainty of the number of events with in the control region. This is propagated through the measurement via the bootstrap method~\cite{bootstrap_method}.
    \item The dominant uncertainty in the mid-range region 150 < \mFourL < 350 GeV are the theory uncertainties associated with the subtraction of prompt-leptons in the control region. These come primarily from QCD scale variations in $WZ$ events. 
    \item A smaller contribution comes from the uncertainties in the Monte-Carlo predictions. This covers the modelling of prompt baseline-not-signal leptons, which get subtracted from the background estimation.  
    \item Fourth is the statistical uncertainty in the control region data used for the calculation of the fake factor. This contribution is subdominant. 
    \item Lastly there is a very small uncertainty from the arbitrary choice of number of intermediate bins used in the background smoothing procedure. It is accounted for by comparing the nominal prediction using 500 bins with alternate predictions using 250 or 1000 bins. 
\end{enumerate}
